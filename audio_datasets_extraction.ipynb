{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Audio Analysis – Preprocessing Assignment***\n",
        "**Submitted by: K. Yugavardhan**\n",
        "\n",
        "**Date: 27th November 2025**\n",
        "\n",
        "This assignment focuses on basic audio preprocessing techniques as part of the broader objective:\n",
        "Automatic Podcast Transcription and Topic Segmentation.\n",
        "\n",
        "The steps covered in this assignment include:\n",
        "\n",
        "* Collection of Audio Data\n",
        "\n",
        "* Load & Inspect Audio Files\n",
        "\n",
        "* Resampling to 16 kHz\n",
        "\n",
        "* Convert Stereo to Mono\n",
        "\n",
        "* Amplitude Normalisation\n",
        "\n",
        "* Noise Reduction\n",
        "\n",
        "* Silence Removal\n",
        "\n",
        "* Voice Activity Detection (VAD)"
      ],
      "metadata": {
        "id": "ddw7Do8VBXB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Environment"
      ],
      "metadata": {
        "id": "CTNWYKUT7EdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run once in Colab\n",
        "!pip install -q mlcroissant librosa soundfile noisereduce webrtcvad tqdm matplotlib\n",
        "!apt-get -qq install -y ffmpeg"
      ],
      "metadata": {
        "id": "P4mqAUiA6ec2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Load & Prepare an Audio Dataset"
      ],
      "metadata": {
        "id": "BZM8kT8X7LRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZsnCZFqKBCvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZmTPZ9PX5mdM"
      },
      "outputs": [],
      "source": [
        "# Use the CROISSANT_URL you set earlier; fallback to manual upload if needed.\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import mlcroissant as mlc\n",
        "import os\n",
        "\n",
        "WORKDIR = Path(\"/content/ljspeech_croissant\")\n",
        "RAW_DIR = WORKDIR / \"raw\"\n",
        "OUT_DIR = WORKDIR / \"processed\"\n",
        "WAV_OUT_DIR = OUT_DIR / \"wavs\"\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "WAV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "USE_CROISSANT = True   # set to False if you want to upload files manually\n",
        "\n",
        "if USE_CROISSANT:\n",
        "    print(\"Loading Croissant dataset from URL...\")\n",
        "    ds = mlc.Dataset(\"https://www.kaggle.com/datasets/mathurinache/the-lj-speech-dataset/croissant/download\")\n",
        "    print(\"Record sets:\", [rs.name for rs in ds.metadata.record_sets])\n",
        "    rs_uuid = ds.metadata.record_sets[0].uuid\n",
        "    # download assets into RAW_DIR (will create the files locally)\n",
        "    ds.download(output_dir=str(RAW_DIR))\n",
        "    df = pd.DataFrame(ds.records(record_set=rs_uuid))\n",
        "    print(\"Metadata loaded. Records:\", len(df))\n",
        "else:\n",
        "    # Manual fallback — upload files via Colab UI and build a simple dataframe\n",
        "    # After uploading, set RAW_DIR to the folder containing .wav files.\n",
        "    import glob\n",
        "    wavs = glob.glob(\"/content/*.wav\")\n",
        "    df = pd.DataFrame({\"audio\": [Path(p).name for p in wavs]})\n",
        "    print(\"Manual mode - found files:\", len(df))\n",
        "\n",
        "# Inspect dataframe head\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.1 — Load Croissant dataset metadata and download audio"
      ],
      "metadata": {
        "id": "-KU1SHi_-eRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "import webrtcvad\n",
        "from tqdm import tqdm\n",
        "\n",
        "TARGET_SR = 16000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ePuWBqT--fTm",
        "outputId": "f5fdff24-cda9-4428-c22e-aa735a0a9c2e"
      },
      "execution_count": 4,
    
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: load"
      ],
      "metadata": {
        "id": "sNpufB-R-_bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_audio(path):\n",
        "    y, sr = sf.read(path)\n",
        "    # ensure float32\n",
        "    if y.dtype != np.float32:\n",
        "        y = y.astype(np.float32)\n",
        "    return y, sr"
      ],
      "metadata": {
        "id": "aFAgxSZdAXtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: resample -> 16kHz"
      ],
      "metadata": {
        "id": "ajvN-g4iAfy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_audio(y, orig_sr, target_sr=TARGET_SR):\n",
        "    if orig_sr == target_sr:\n",
        "        return y\n",
        "    # librosa expects 1D; if stereo handle each channel\n",
        "    if y.ndim == 1:\n",
        "        return librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr)\n",
        "    else:\n",
        "        # y shape could be (n_samples, channels)\n",
        "        return np.mean([librosa.resample(y[:,ch], orig_sr=orig_sr, target_sr=target_sr) for ch in range(y.shape[1])], axis=0)"
      ],
      "metadata": {
        "id": "7C3zMER_AdFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: convert to mono"
      ],
      "metadata": {
        "id": "mfvZgr1_AkGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_mono(y):\n",
        "    if y.ndim == 1:\n",
        "        return y\n",
        "    # (n_samples, channels) -> mean across channels\n",
        "    return np.mean(y, axis=1)"
      ],
      "metadata": {
        "id": "pwjg1IvxAkQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: normalize to -1..+1 (peak normalization)"
      ],
      "metadata": {
        "id": "mqwdtC3oAuNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_audio(y):\n",
        "    return librosa.util.normalize(y)"
      ],
      "metadata": {
        "id": "bo_YxUeQAtI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: noise reduction (spectral gating via noisereduce)"
      ],
      "metadata": {
        "id": "gteREM0QAzHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def denoise_audio(y, sr=TARGET_SR):\n",
        "    try:\n",
        "        return nr.reduce_noise(y=y, sr=sr)\n",
        "    except Exception as e:\n",
        "        # if it fails, return original\n",
        "        print(\"noisereduce failed:\", e)\n",
        "        return y"
      ],
      "metadata": {
        "id": "uXL8V-tOAyjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 6: trim leading/trailing silence"
      ],
      "metadata": {
        "id": "GbDNXjsfA5OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_silence(y, top_db=20):\n",
        "    y_trim, _ = librosa.effects.trim(y, top_db=top_db)\n",
        "    return y_trim"
      ],
      "metadata": {
        "id": "Vw9AMg0JA8Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: VAD using WebRTC VAD - returns concatenated voiced regions (float array)"
      ],
      "metadata": {
        "id": "OCL8T_rgBBrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vad_concat(y, sr=TARGET_SR, aggressiveness=2, frame_ms=30):\n",
        "    # webrtcvad works on 16-bit PCM bytes and supports sample rates 8000,16000,32000,48000\n",
        "    assert sr in (8000,16000,32000,48000), \"webrtcvad supports 8/16/32/48 kHz only\"\n",
        "    vad = webrtcvad.Vad(aggressiveness)\n",
        "    frame_len = int(sr * frame_ms / 1000)  # samples per frame\n",
        "    pcm16 = (y * 32767).astype(np.int16)\n",
        "\n",
        "    segments = []\n",
        "    start = None\n",
        "    for offset in range(0, len(pcm16) - frame_len + 1, frame_len):\n",
        "        frame_bytes = pcm16[offset:offset+frame_len].tobytes()\n",
        "        is_speech = vad.is_speech(frame_bytes, sr)\n",
        "        if is_speech and start is None:\n",
        "            start = offset\n",
        "        if (not is_speech) and start is not None:\n",
        "            segments.append((start, offset))\n",
        "            start = None\n",
        "    if start is not None:\n",
        "        segments.append((start, len(pcm16)))\n",
        "\n",
        "    if not segments:\n",
        "        return y  # no voiced segments found, return original\n",
        "    voiced = np.concatenate([y[s:e] for s,e in segments])\n",
        "    return voiced"
      ],
      "metadata": {
        "id": "zYeyYp5IA-nX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
